#!/usr/bin/env python3
"""
æµ‹è¯•æ›´æ–°åçš„ MMAE è®­ç»ƒåŠŸèƒ½
"""

import torch
from src.hook import train_mmae


def test_mmae_training():
    """æµ‹è¯• MMAE è®­ç»ƒçš„åŸºæœ¬åŠŸèƒ½"""
    print("=== æµ‹è¯•æ›´æ–°åçš„ MMAE è®­ç»ƒ ===")

    # ä½¿ç”¨å°è§„æ¨¡å‚æ•°è¿›è¡Œå¿«é€Ÿæµ‹è¯•
    try:
        results = train_mmae(
            epochs=2,  # åªè®­ç»ƒ2ä¸ªepoch
            lr=1e-4,
            batch_size=4,  # å°batch size
            weight_decay=1e-4,
            seed=42,
            image_size=32,  # å°å›¾åƒå°ºå¯¸
            data_root="/data/SSD/coco/images/",
            num_workers=0,  # é¿å…å¤šè¿›ç¨‹é—®é¢˜
            temperature=0.07,
            backbone_vision=None,  # ä½¿ç”¨è‡ªå®šä¹‰ç¼–ç å™¨
            text_backbone="bert-base-uncased",
            text_max_len=32,
            proj_dim=128,
            mae_weight=1.0,
            mlm_weight=1.0,
            contrastive_weight=1.0,
            save_dir=None,  # ä¸ä¿å­˜æ¨¡å‹
            save_interval=1,
            logger=None,  # ä¸ä½¿ç”¨wandb
        )

        print("\nâœ… è®­ç»ƒå®Œæˆï¼")
        print("=== è®­ç»ƒç»“æœ ===")
        print(f"è®­ç»ƒè½®æ•°: {len(results['train_total_losses'])}")
        print(f"éªŒè¯è½®æ•°: {len(results['val_total_losses'])}")
        print(f"æœ€ç»ˆè®­ç»ƒæŸå¤±: {results['train_total_losses'][-1]:.4f}")
        print(f"æœ€ç»ˆéªŒè¯æŸå¤±: {results['val_total_losses'][-1]:.4f}")
        print(f"æµ‹è¯•æ€»æŸå¤±: {results['test_total_loss']:.4f}")
        print(f"æµ‹è¯•MAEæŸå¤±: {results['test_mae_loss']:.4f}")
        print(f"æµ‹è¯•MLMæŸå¤±: {results['test_mlm_loss']:.4f}")
        print(f"æµ‹è¯•å¯¹æ¯”æŸå¤±: {results['test_contrastive_loss']:.4f}")
        print(f"æœ€ç»ˆå­¦ä¹ ç‡: {results['last_lr']:.6f}")

    except Exception as e:
        print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback

        traceback.print_exc()
        return False

    return True


def test_wandb_logging():
    """æµ‹è¯• wandb æ—¥å¿—è®°å½•ï¼ˆæ¨¡æ‹Ÿï¼‰"""
    print("\n=== æµ‹è¯• Wandb æ—¥å¿—è®°å½• ===")

    # æ¨¡æ‹Ÿè®­ç»ƒç»“æœ
    mock_results = {
        "train_total_losses": [0.5, 0.4],
        "val_total_losses": [0.6, 0.5],
        "train_mae_losses": [0.2, 0.15],
        "train_mlm_losses": [0.15, 0.12],
        "train_contrastive_losses": [0.15, 0.13],
        "val_mae_losses": [0.25, 0.2],
        "val_mlm_losses": [0.2, 0.18],
        "val_contrastive_losses": [0.15, 0.12],
        "test_total_loss": 0.45,
        "test_mae_loss": 0.18,
        "test_mlm_loss": 0.15,
        "test_contrastive_loss": 0.12,
        "last_lr": 1e-5,
    }

    print("æ¨¡æ‹Ÿçš„ wandb æŒ‡æ ‡:")
    print("è®­ç»ƒæŒ‡æ ‡:")
    for i, (tr, va) in enumerate(
        zip(mock_results["train_total_losses"], mock_results["val_total_losses"]), 1
    ):
        print(f"  Epoch {i}: train={tr:.4f}, val={va:.4f}")

    print("æµ‹è¯•æŒ‡æ ‡:")
    print(f"  test_total_loss: {mock_results['test_total_loss']:.4f}")
    print(f"  test_mae_loss: {mock_results['test_mae_loss']:.4f}")
    print(f"  test_mlm_loss: {mock_results['test_mlm_loss']:.4f}")
    print(f"  test_contrastive_loss: {mock_results['test_contrastive_loss']:.4f}")

    print("âœ… Wandb æ—¥å¿—è®°å½•æµ‹è¯•å®Œæˆ")


if __name__ == "__main__":
    print("å¼€å§‹æµ‹è¯•æ›´æ–°åçš„ MMAE åŠŸèƒ½...")

    # æµ‹è¯•è®­ç»ƒåŠŸèƒ½
    success = test_mmae_training()

    if success:
        # æµ‹è¯• wandb æ—¥å¿—è®°å½•
        test_wandb_logging()
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
    else:
        print("\nâŒ æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
